{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 압축해제해야 될 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tar_files> \n",
      "['downloads2/src\\\\arXiv_src_0001_001.tar', 'downloads2/src\\\\arXiv_src_0002_001.tar', 'downloads2/src\\\\arXiv_src_0003_001.tar', 'downloads2/src\\\\arXiv_src_0004_001.tar', 'downloads2/src\\\\arXiv_src_0005_001.tar', 'downloads2/src\\\\arXiv_src_0006_001.tar', 'downloads2/src\\\\arXiv_src_0007_001.tar', 'downloads2/src\\\\arXiv_src_0008_001.tar', 'downloads2/src\\\\arXiv_src_0009_001.tar', 'downloads2/src\\\\arXiv_src_0010_001.tar']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import gzip\n",
    "import re\n",
    "import io\n",
    "import chardet\n",
    "import json\n",
    "\n",
    "def extract_tar_files(src_folder):\n",
    "    tar_files = [os.path.join(src_folder, file) for file in os.listdir(src_folder) if file.endswith('.tar')]\n",
    "    print(f\"<tar_files> \\n{tar_files}\")\n",
    "    return tar_files\n",
    "\n",
    "def extract_gz_from_tar(tar_file):\n",
    "    extracted_files = []\n",
    "    with tarfile.open(tar_file, 'r') as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if member.isfile() and member.name.endswith('.gz'):\n",
    "                extracted_files.append(tar.extractfile(member).read())\n",
    "    print(f\"gz files : \\n {extracted_files}\")\n",
    "    return extracted_files\n",
    "\n",
    "def detect_encoding(data):\n",
    "    result = chardet.detect(data)\n",
    "    encoding = result['encoding']\n",
    "    if encoding is None:\n",
    "        return 'utf-8'  # 기본 인코딩 설정\n",
    "    return encoding\n",
    "\n",
    "def extract_tex_from_gz(gz_data):\n",
    "    tex_content = \"\"\n",
    "    with gzip.open(io.BytesIO(gz_data), 'rb') as f:\n",
    "        raw_data = f.read()\n",
    "        encoding = detect_encoding(raw_data)\n",
    "        try:\n",
    "            tex_content = raw_data.decode(encoding)\n",
    "        except (UnicodeDecodeError, TypeError):\n",
    "            tex_content = raw_data.decode('latin1')  # utf-8 디코딩 실패시 기본적으로 latin1 사용\n",
    "    return tex_content\n",
    "\n",
    "def extract_equations_with_context(tex_content):\n",
    "    pattern = r'(?P<context_before>.{0,200}?)(?P<equation>\\$.*?\\$)(?P<context_after>.{0,200}?)($|\\n)'\n",
    "    matches = re.finditer(pattern, tex_content, re.DOTALL)\n",
    "    equations_with_context = []\n",
    "    for match in matches:\n",
    "        context_before = match.group('context_before').strip()\n",
    "        equation = match.group('equation').strip()\n",
    "        context_after = match.group('context_after').strip()\n",
    "\n",
    "        # context_before를 10단어 이하로 제한\n",
    "        context_before_words = context_before.split()\n",
    "        if len(context_before_words) > 10:\n",
    "            context_before = ' '.join(context_before_words[-10:])\n",
    "        \n",
    "        # context_after를 10단어 이하로 제한\n",
    "        context_after_words = context_after.split()\n",
    "        if len(context_after_words) > 10:\n",
    "            context_after = ' '.join(context_after_words[:10])\n",
    "        \n",
    "        # context_after에 수식이 없도록 처리\n",
    "        if '$' in context_after:\n",
    "            context_after = context_after.split('$')[0].strip()\n",
    "                \n",
    "        # context_after가 비어 있는 경우 다음 줄의 내용을 포함하도록 보정\n",
    "        if not context_after and '\\n' in tex_content:\n",
    "            remaining_text = tex_content.split(equation, 1)[1].strip()\n",
    "            context_after = remaining_text.split('\\n', 1)[0].strip()\n",
    "            if '$' in context_after:\n",
    "                context_after = context_after.split('$')[0].strip()\n",
    "\n",
    "        # context_after를 다시 10단어 이하로 제한\n",
    "        context_after_words = context_after.split()\n",
    "        if len(context_after_words) > 10:\n",
    "            context_after = ' '.join(context_after_words[:10])\n",
    "\n",
    "        # 수식의 길이가 100 이하인지 확인\n",
    "        if len(equation) <= 100:\n",
    "            equations_with_context.append({\n",
    "                'context_before': context_before,\n",
    "                'equation': f'<equation>{equation}</equation>',\n",
    "                'context_after': context_after\n",
    "            })\n",
    "    return equations_with_context\n",
    "\n",
    "def save_results_to_jsonl(results, output_file):\n",
    "    with open(output_file, 'a', encoding='utf-8') as file:  # 'a' 모드를 사용하여 파일에 추가\n",
    "        for result in results:\n",
    "            file.write(json.dumps(result) + '\\n')\n",
    "\n",
    "def process_gz_file(gz_data, output_file):\n",
    "    tex_content = extract_tex_from_gz(gz_data)\n",
    "    equations_with_context = extract_equations_with_context(tex_content)\n",
    "    if len(equations_with_context) > 10000:\n",
    "        return\n",
    "    save_results_to_jsonl(equations_with_context, output_file)\n",
    "\n",
    "def main(src_folder, output_file):\n",
    "    tar_files = extract_tar_files(src_folder)\n",
    "    \n",
    "    for tar_file in tar_files:\n",
    "        gz_files = extract_gz_from_tar(tar_file)\n",
    "        for gz_data in gz_files:\n",
    "            process_gz_file(gz_data, output_file)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    src_folder = 'downloads2/src'  # src 폴더의 경로를 적절히 설정하세요.\n",
    "    output_file = 'output_example.jsonl'\n",
    "    main(src_folder, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tex 파일만 있을 때"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_equations_with_context(tex_content):\n",
    "    pattern = r'(?P<context_before>.{0,200}?)(?P<equation>\\$.*?\\$)(?P<context_after>.{0,200}?)($|\\n)'\n",
    "    matches = re.finditer(pattern, tex_content, re.DOTALL)\n",
    "    equations_with_context = []\n",
    "    for match in matches:\n",
    "        context_before = match.group('context_before').strip()\n",
    "        equation = match.group('equation').strip()\n",
    "        context_after = match.group('context_after').strip()\n",
    "\n",
    "        # context_before를 10단어 이하로 제한\n",
    "        context_before_words = context_before.split()\n",
    "        if len(context_before_words) > 10:\n",
    "            context_before = ' '.join(context_before_words[-10:])\n",
    "        \n",
    "        # context_after를 10단어 이하로 제한\n",
    "        context_after_words = context_after.split()\n",
    "        if len(context_after_words) > 10:\n",
    "            context_after = ' '.join(context_after_words[:10])\n",
    "        \n",
    "        # context_after에 수식이 없도록 처리\n",
    "        if '$' in context_after:\n",
    "            context_after = context_after.split('$')[0].strip()\n",
    "                \n",
    "        # context_after가 비어 있는 경우 다음 줄의 내용을 포함하도록 보정\n",
    "        if not context_after and '\\n' in tex_content:\n",
    "            remaining_text = tex_content.split(equation, 1)[1].strip()\n",
    "            context_after = remaining_text.split('\\n', 1)[0].strip()\n",
    "            if '$' in context_after:\n",
    "                context_after = context_after.split('$')[0].strip()\n",
    "\n",
    "        # context_after를 다시 10단어 이하로 제한\n",
    "        context_after_words = context_after.split()\n",
    "        if len(context_after_words) > 10:\n",
    "            context_after = ' '.join(context_after_words[:10])\n",
    "\n",
    "        # 수식의 길이가 100 이하인지 확인\n",
    "        if len(equation) <= 100:\n",
    "            equations_with_context.append({\n",
    "                'context_before': context_before,\n",
    "                'equation': f'<equation>{equation}</equation>',\n",
    "                'context_after': context_after\n",
    "            })\n",
    "    return equations_with_context\n",
    "\n",
    "# Path to the arxiv_papers_tex folder\n",
    "base_path = 'C:/Users/wjdrb/vscode_code/MathBridge_new/data/arxiv_papers_tex'\n",
    "\n",
    "\n",
    "# List to store all equations with context\n",
    "all_equations_with_context = []\n",
    "\n",
    "# Function to read file with multiple encoding attempts\n",
    "def read_file_with_multiple_encodings(file_path):\n",
    "    encodings = ['utf-8', 'latin-1', 'iso-8859-1']\n",
    "    for encoding in encodings:\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding=encoding) as file:\n",
    "                return file.read()\n",
    "        except (UnicodeDecodeError, FileNotFoundError):\n",
    "            continue\n",
    "    raise UnicodeDecodeError(f\"Failed to read file {file_path} with available encodings.\")\n",
    "\n",
    "# Iterate over each folder (paper) in the base directory\n",
    "for paper_folder in os.listdir(base_path):\n",
    "    paper_folder_path = os.path.join(base_path, paper_folder)\n",
    "    \n",
    "    if os.path.isdir(paper_folder_path):\n",
    "        # Iterate over each file in the paper folder\n",
    "        for tex_file in os.listdir(paper_folder_path):\n",
    "            if tex_file.endswith('.tex') or tex_file.endswith('.TEX'):\n",
    "                tex_file_path = os.path.join(paper_folder_path, tex_file)\n",
    "                \n",
    "                # Read the content of the tex file with multiple encoding attempts\n",
    "                try:\n",
    "                    tex_content = read_file_with_multiple_encodings(tex_file_path)\n",
    "                except UnicodeDecodeError as e:\n",
    "                    print(f\"Could not read file {tex_file_path}: {e}\")\n",
    "                    continue\n",
    "                \n",
    "                # Extract equations with context\n",
    "                equations_with_context = extract_equations_with_context(tex_content)\n",
    "                \n",
    "                # Add the paper number to each extracted entry\n",
    "                for entry in equations_with_context:\n",
    "                    entry['paper_number'] = paper_folder\n",
    "                \n",
    "                # Append to the list\n",
    "                all_equations_with_context.extend(equations_with_context)\n",
    "\n",
    "# Define the output file path\n",
    "output_file_path = 'equations_with_context_math.jsonl'\n",
    "\n",
    "# Write the results to a jsonl file\n",
    "with open(output_file_path, 'w', encoding='utf-8') as outfile:\n",
    "    for entry in all_equations_with_context:\n",
    "        json.dump(entry, outfile)\n",
    "        outfile.write('\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
