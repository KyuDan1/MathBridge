{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def clean_gpu_memory():\n",
    "    torch.cuda.empty_cache()\n",
    "    torch.cuda.ipc_collect()\n",
    "\n",
    "# Usage example\n",
    "clean_gpu_memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.64s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The LaTeX code \"\\frac{{-b \\pm \\sqrt{{b^2 - 4ac}}}}{2a}\" translates to:\n",
      "\n",
      "\"X equals negative b, plus or minus the square root of b-squared minus 4a-c, all divided by 2a.\"\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "pipe = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model_id,\n",
    "    model_kwargs={\n",
    "        \"torch_dtype\": torch.float16,\n",
    "        \"quantization_config\": {\"load_in_4bit\": True},\n",
    "        \"low_cpu_mem_usage\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator who translate LaTeX into spoken English.\"},\n",
    "    {\"role\": \"user\", \"content\": r\"Translate the LaTeX into spoken English: x = \\frac{{-b \\pm \\sqrt{{b^2 - 4ac}}}}{2a}\"},\n",
    "]\n",
    "\n",
    "terminators = [\n",
    "    pipe.tokenizer.eos_token_id,\n",
    "    pipe.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "]\n",
    "\n",
    "outputs = pipe(\n",
    "    messages,\n",
    "    max_new_tokens=256,\n",
    "    eos_token_id=terminators,\n",
    "    do_sample=True,\n",
    "    temperature=0.6,\n",
    "    top_p=0.9,\n",
    ")\n",
    "assistant_response = outputs[0][\"generated_text\"][-1][\"content\"]\n",
    "print(assistant_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:59: SyntaxWarning: invalid escape sequence '\\ '\n",
      "<>:59: SyntaxWarning: invalid escape sequence '\\ '\n",
      "C:\\Users\\wjdrb\\AppData\\Local\\Temp\\ipykernel_53464\\3147869073.py:59: SyntaxWarning: invalid escape sequence '\\ '\n",
      "  \"\"\"\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:27<00:00,  6.89s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:33<00:00,  8.37s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X equals negative B, plus or minus, the square root of B squared minus 4AC, all divided by 2A.\n",
      "x equals negative b, plus or minus the square root of b-squared minus 4ac, all divided by 2a\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import concurrent.futures\n",
    "\n",
    "# 모델 초기화 함수\n",
    "def initialize_pipeline(model_id):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"quantization_config\": {\"load_in_4bit\": True},\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# 모델 ID\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# 메시지 및 종료 토큰 설정\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a translator who translate LaTeX into spoken English.\"},\n",
    "    {\"role\": \"user\", \"content\": r\"Translate the LaTeX into spoken English: x = \\frac{{-b \\pm \\sqrt{{b^2 - 4ac}}}}{2a}\"},\n",
    "]\n",
    "\n",
    "def translate_with_pipeline(pipe, messages):\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"]\n",
    "\n",
    "# 파이프라인 초기화\n",
    "num_pipelines = 2  # 병렬로 사용할 모델 파이프라인의 수\n",
    "pipelines = [initialize_pipeline(model_id) for _ in range(num_pipelines)]\n",
    "\n",
    "def find_assistant_message(messages):\n",
    "    for message in messages:\n",
    "        if message['role'] == 'assistant':\n",
    "            return message['content']\n",
    "    return None\n",
    "\n",
    "def extract_only_data(data):\n",
    "    if '\"' in data:\n",
    "        parts = data.split('\"')\n",
    "        result = parts[-2] if len(parts) >= 3 else data\n",
    "    elif '\\n' in data:\n",
    "        lines = data.split('\\n')\n",
    "        result = lines[-1] if lines else data\n",
    "        \n",
    "    elif 'sorry' in data or 'cannot' in data or 'apologize' in data or '$' in data:\n",
    "        result = \"None\"\n",
    "    else:\n",
    "        result = data\n",
    "    return result\n",
    "\n",
    "# 병렬 처리\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_pipelines) as executor:\n",
    "    future_to_pipe = {executor.submit(translate_with_pipeline, pipe, messages): pipe for pipe in pipelines}\n",
    "    for future in concurrent.futures.as_completed(future_to_pipe):\n",
    "        pipe = future_to_pipe[future]\n",
    "        try:\n",
    "            result = future.result()\n",
    "            result = find_assistant_message(result)\n",
    "            result = extract_only_data(result)\n",
    "            print(result)\n",
    "        except Exception as exc:\n",
    "            print(f'{pipe} generated an exception: {exc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load complete\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet('df_not_len_5_cleaned_unique_eq.parquet', engine='fastparquet')\n",
    "print(\"Load complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.69s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:14<00:00,  3.55s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import concurrent.futures\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "\n",
    "# 모델 초기화 함수\n",
    "def initialize_pipeline(model_id):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"quantization_config\": {\"load_in_4bit\": True},\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# 번역 함수\n",
    "def translate_with_pipeline(pipe, text):\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "    ]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a translator who translate LaTeX into spoken English.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Translate the LaTeX into spoken English: {text}\"},\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"]\n",
    "\n",
    "# 어시스턴트 메시지 추출 함수\n",
    "def find_assistant_message(messages):\n",
    "    for message in messages:\n",
    "        if message['role'] == 'assistant':\n",
    "            return message['content']\n",
    "    return None\n",
    "\n",
    "# 데이터 추출 함수\n",
    "def extract_only_data(data):\n",
    "    if '\"' in data:\n",
    "        parts = data.split('\"')\n",
    "        result = parts[-2] if len(parts) >= 3 else data\n",
    "    elif '\\n' in data:\n",
    "        lines = data.split('\\n')\n",
    "        result = lines[-1] if lines else data\n",
    "    elif 'sorry' in data or 'cannot' in data or 'apologize' in data or '$' in data:\n",
    "        result = \"None\"\n",
    "    else:\n",
    "        result = data\n",
    "    return result\n",
    "\n",
    "# 병렬로 번역하고 결과를 저장하는 함수\n",
    "def process_and_save(df, pipe, file_idx, times):\n",
    "    results = []\n",
    "    for eq in df['equation']:\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            result = translate_with_pipeline(pipe, eq)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            times.append(elapsed_time)\n",
    "            result = find_assistant_message(result)\n",
    "            result = extract_only_data(result)\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            results.append(None)\n",
    "            print(f\"Error processing equation {eq}: {e}\")\n",
    "\n",
    "    df['translated'] = results\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'translated_part_{file_idx}.parquet')\n",
    "\n",
    "# 데이터프레임을 파이프라인 개수로 나누는 함수\n",
    "def split_dataframe(df, n):\n",
    "    return [df[i::n] for i in range(n)]\n",
    "\n",
    "# 모델 ID 및 파이프라인 초기화\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "num_pipelines = 2\n",
    "pipelines = [initialize_pipeline(model_id) for _ in range(num_pipelines)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 데이터프레임 로드\n",
    "df = pd.read_parquet('df_not_len_5_cleaned_unique_eq.parquet', engine='fastparquet')\n",
    "\n",
    "# 데이터프레임 나누기\n",
    "dfs = split_dataframe(df, num_pipelines)\n",
    "\n",
    "# 평균 시간을 저장할 리스트\n",
    "times = []\n",
    "\n",
    "# 병렬 처리 및 저장\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=num_pipelines) as executor:\n",
    "    futures = []\n",
    "    for i, (pipe, df_part) in enumerate(zip(pipelines, dfs)):\n",
    "        futures.append(executor.submit(process_and_save, df_part, pipe, i, times))\n",
    "\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        try:\n",
    "            future.result()\n",
    "        except Exception as exc:\n",
    "            print(f'Generated an exception: {exc}')\n",
    "\n",
    "# 예상 완료 시간 계산\n",
    "if times:\n",
    "    avg_time_per_eq = sum(times) / len(times)\n",
    "    total_eq = len(df)\n",
    "    total_time = avg_time_per_eq * total_eq\n",
    "    print(f\"Estimated completion time: {total_time / 60:.2f} minutes\")\n",
    "else:\n",
    "    print(\"No timings collected, cannot estimate completion time.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:10<00:00,  2.50s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.12s/it]\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n",
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\bitsandbytes\\nn\\modules.py:391: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn('Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.')\n",
      "c:\\Users\\wjdrb\\vscode_code\\venv\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:648: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is in the interval 3, 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma squared t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A function of t.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X naught is equal to X of t.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta t\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow as pa\n",
    "import transformers\n",
    "import torch\n",
    "import time\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import nest_asyncio\n",
    "\n",
    "# 이벤트 루프 중첩을 허용\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# 모델 초기화 함수\n",
    "def initialize_pipeline(model_id):\n",
    "    return transformers.pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model_id,\n",
    "        model_kwargs={\n",
    "            \"torch_dtype\": torch.float16,\n",
    "            \"quantization_config\": {\"load_in_4bit\": True},\n",
    "            \"low_cpu_mem_usage\": True,\n",
    "        },\n",
    "    )\n",
    "\n",
    "# 번역 함수\n",
    "def translate_with_pipeline(pipe, text):\n",
    "    terminators = [\n",
    "        pipe.tokenizer.eos_token_id,\n",
    "    ]\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a translator who translate LaTeX into spoken English.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"Translate the LaTeX into spoken English: {text}\"},\n",
    "    ]\n",
    "    outputs = pipe(\n",
    "        messages,\n",
    "        max_new_tokens=256,\n",
    "        eos_token_id=terminators,\n",
    "        do_sample=True,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    return outputs[0][\"generated_text\"]\n",
    "\n",
    "# 어시스턴트 메시지 추출 함수\n",
    "def find_assistant_message(messages):\n",
    "    for message in messages:\n",
    "        if message['role'] == 'assistant':\n",
    "            return message['content']\n",
    "    return None\n",
    "\n",
    "# 데이터 추출 함수\n",
    "def extract_only_data(data):\n",
    "    if '\"' in data:\n",
    "        parts = data.split('\"')\n",
    "        result = parts[-2] if len(parts) >= 3 else data\n",
    "    elif '\\n' in data:\n",
    "        lines = data.split('\\n')\n",
    "        result = lines[-1] if lines else data\n",
    "    elif 'sorry' in data or 'cannot' in data or 'apologize' in data or '$' in data:\n",
    "        result = \"None\"\n",
    "    else:\n",
    "        result = data\n",
    "    return result\n",
    "\n",
    "# 병렬로 번역하고 결과를 저장하는 함수\n",
    "async def process_and_save(df, pipe, file_idx, times, executor):\n",
    "    loop = asyncio.get_event_loop()\n",
    "    results = []\n",
    "\n",
    "    for eq in df['equation']:\n",
    "        start_time = time.time()\n",
    "        result = await loop.run_in_executor(executor, translate_with_pipeline, pipe, eq)\n",
    "        elapsed_time = time.time() - start_time\n",
    "        times.append(elapsed_time)\n",
    "\n",
    "        result = find_assistant_message(result)\n",
    "        result = extract_only_data(result)\n",
    "        results.append(result)\n",
    "        print(result)  # 생성된 텍스트 출력\n",
    "\n",
    "    df['translated'] = results\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_table(table, f'translated_part_{file_idx}.parquet')\n",
    "\n",
    "# 데이터프레임을 파이프라인 개수로 나누는 함수\n",
    "def split_dataframe(df, n):\n",
    "    return [df[i::n] for i in range(n)]\n",
    "\n",
    "# 메인 함수\n",
    "async def main():\n",
    "    model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "    num_pipelines = 2\n",
    "    pipelines = [initialize_pipeline(model_id) for _ in range(num_pipelines)]\n",
    "\n",
    "    df = pd.read_parquet('df_not_len_5_cleaned_unique_eq.parquet', engine='fastparquet')\n",
    "    dfs = split_dataframe(df, num_pipelines)\n",
    "\n",
    "    times = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_pipelines) as executor:\n",
    "        tasks = [\n",
    "            process_and_save(df_part, pipe, i, times, executor)\n",
    "            for i, (pipe, df_part) in enumerate(zip(pipelines, dfs))\n",
    "        ]\n",
    "        await asyncio.gather(*tasks)\n",
    "\n",
    "    if times:\n",
    "        avg_time_per_eq = sum(times) / len(times)\n",
    "        total_eq = len(df)\n",
    "        total_time = avg_time_per_eq * total_eq\n",
    "        print(f\"Estimated completion time: {total_time / 60:.2f} minutes\")\n",
    "    else:\n",
    "        print(\"No timings collected, cannot estimate completion time.\")\n",
    "\n",
    "# 실행\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
