{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow nested use of asyncio.run()\n",
    "nest_asyncio.apply()\n",
    "# Load dataset\n",
    "dataset = pd.read_parquet('T5_allocate.parquet', engine='fastparquet')\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define model paths\n",
    "model_paths = [\"Hyeonsieun/GTtoNT_addmoretoken_ver2\"] * 25\n",
    "\n",
    "\n",
    "# Load models and tokenizers\n",
    "models = []\n",
    "tokenizers = []\n",
    "for path in model_paths:\n",
    "    tokenizer = T5Tokenizer.from_pretrained(path)\n",
    "    model = T5ForConditionalGeneration.from_pretrained(path).to(device)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "    tokenizers.append(tokenizer)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def do_correction_3(text, model, tokenizer):\n",
    "    input_text = f\"translate the LaTeX equation to a text pronouncing the formula: {text}\"\n",
    "    inputs = tokenizer.encode(\n",
    "        input_text,\n",
    "        return_tensors='pt',\n",
    "        max_length=325,\n",
    "        padding='max_length',\n",
    "        truncation=True\n",
    "    ).to(device)\n",
    "\n",
    "    corrected_ids = model.generate(\n",
    "        inputs,\n",
    "        max_length=325,\n",
    "        num_beams=5,\n",
    "        early_stopping=True\n",
    "    )\n",
    "\n",
    "    corrected_sentence = tokenizer.decode(\n",
    "        corrected_ids[0],\n",
    "        skip_special_tokens=False\n",
    "    )\n",
    "    start_index = corrected_sentence.find(\"<pad>\") + len(\"<pad>\")\n",
    "    end_index = corrected_sentence.find(\"</s>\")\n",
    "    corrected_sentence = corrected_sentence[start_index:end_index].strip()\n",
    "    corrected_sentence = corrected_sentence.replace(\"<unk>\", \"\")\n",
    "    return corrected_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import asyncio\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import asyncio\n",
    "\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "def process_batch(start_idx, end_idx, model, tokenizer, dataset, batch_num):\n",
    "    NT = []\n",
    "    for i in range(start_idx, end_idx):\n",
    "        TeX = dataset['equation'][i]\n",
    "        print(f\"{i} input : {TeX}\")\n",
    "        NT_result = asyncio.run(do_correction_3(TeX, model, tokenizer))\n",
    "        print(f\"{i} result : {NT_result}\")\n",
    "        NT.append(NT_result)\n",
    "        if len(NT) >= 200:\n",
    "            # Save intermediate results\n",
    "            save_partial_results(NT, batch_num, i)\n",
    "            NT = []\n",
    "    # Save any remaining results\n",
    "    if NT:\n",
    "        save_partial_results(NT, batch_num, end_idx - 1)\n",
    "\n",
    "def save_partial_results(NT, batch_num, end_idx):\n",
    "    df_partial = pd.DataFrame(NT, columns=['spoken_English'])\n",
    "    filename = f\"parquets/NT_results_batch{batch_num}_part{end_idx}.parquet\"\n",
    "    df_partial.to_parquet(filename, index=False, engine='fastparquet')\n",
    "    print(f\"Saved partial results to {filename}\")\n",
    "\n",
    "def main():\n",
    "    # Split the dataset indices for each model\n",
    "    num_samples = len(dataset)\n",
    "    num_models = len(models)\n",
    "    batch_size = num_samples // num_models\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=num_models) as executor:\n",
    "        futures = []\n",
    "        for i in range(num_models):\n",
    "            start_idx = i * batch_size\n",
    "            end_idx = (i + 1) * batch_size if i != num_models - 1 else num_samples\n",
    "            futures.append(executor.submit(process_batch, start_idx, end_idx, models[i], tokenizers[i], dataset, i))\n",
    "        \n",
    "        # Wait for all futures to complete\n",
    "        for future in futures:\n",
    "            future.result()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
